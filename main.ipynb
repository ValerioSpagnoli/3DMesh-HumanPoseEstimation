{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.layers.mesh_pool import MeshPool\n",
    "from models.layers.mesh_conv import MeshConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_args(norm_layer, nfeats_list):\n",
    "    if hasattr(norm_layer, '__name__') and norm_layer.__name__ == 'NoNorm':\n",
    "        norm_args = [{'fake': True} for f in nfeats_list]\n",
    "    elif norm_layer.func.__name__ == 'GroupNorm':\n",
    "        norm_args = [{'num_channels': f} for f in nfeats_list]\n",
    "    elif norm_layer.func.__name__ == 'BatchNorm':\n",
    "        norm_args = [{'num_features': f} for f in nfeats_list]\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_layer.func.__name__)\n",
    "    return norm_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshConvNet(nn.Module):\n",
    "    \"\"\"Network for learning a global shape descriptor (classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, norm_layer, nf0, conv_res, nclasses, input_res, pool_res, fc_n, nresblocks=3):\n",
    "        super(MeshConvNet, self).__init__()\n",
    "        self.k = [nf0] + conv_res\n",
    "        self.res = [input_res] + pool_res\n",
    "        norm_args = get_norm_args(norm_layer, self.k[1:])\n",
    "\n",
    "        # for i, ki in enumerate(self.k[:-1]):\n",
    "        #     setattr(self, 'conv{}'.format(i), MResConv(ki, self.k[i + 1], nresblocks))\n",
    "        #     setattr(self, 'norm{}'.format(i), norm_layer(**norm_args[i]))\n",
    "        #     setattr(self, 'pool{}'.format(i), MeshPool(self.res[i + 1]))\n",
    "    \n",
    "        self.conv0 = MResConv(self.k[0], self.k[1], nresblocks) \n",
    "        self.norm0 = norm_layer(**norm_args[0])\n",
    "        self.pool0 = MeshPool(self.res[1])\n",
    "        \n",
    "        self.conv1 = MResConv(self.k[1], self.k[2], nresblocks)\n",
    "        self.norm1 = norm_layer(**norm_args[1])\n",
    "        self.pool1 = MeshPool(self.res[2])\n",
    "        \n",
    "        self.conv2 = MResConv(self.k[2], self.k[3], nresblocks)\n",
    "        self.norm2 = norm_layer(**norm_args[2])\n",
    "        self.pool2 = MeshPool(self.res[3])\n",
    "\n",
    "\n",
    "        self.gp = torch.nn.AvgPool1d(self.res[-1])\n",
    "        self.fc1 = nn.Linear(self.k[-1], fc_n)\n",
    "        self.fc2 = nn.Linear(fc_n, nclasses)\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "\n",
    "        for i in range(len(self.k) - 1):\n",
    "            x = getattr(self, 'conv{}'.format(i))(x, mesh)\n",
    "            x = F.relu(getattr(self, 'norm{}'.format(i))(x))\n",
    "            x = getattr(self, 'pool{}'.format(i))(x, mesh)\n",
    "\n",
    "        x = self.gp(x)\n",
    "        x = x.view(-1, self.k[-1])\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MResConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skips=1):\n",
    "        super(MResConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.skips = skips\n",
    "        self.conv0 = MeshConv(self.in_channels, self.out_channels, bias=False)\n",
    "        for i in range(self.skips):\n",
    "            setattr(self, 'bn{}'.format(i + 1), nn.BatchNorm2d(self.out_channels))\n",
    "            setattr(self, 'conv{}'.format(i + 1), MeshConv(self.out_channels, self.out_channels, bias=False))\n",
    "\n",
    "    def forward(self, x, mesh):\n",
    "        x = self.conv0(x, mesh)\n",
    "        x1 = x\n",
    "        for i in range(self.skips):\n",
    "            x = getattr(self, 'bn{}'.format(i + 1))(F.relu(x))\n",
    "            x = getattr(self, 'conv{}'.format(i + 1))(x, mesh)\n",
    "        x += x1\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU IDs: [0]\n",
      "Batch size: 8\n",
      "loaded mean / std from cache\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.classification_data import ClassificationData  # Custom dataset\n",
    "from data.base_dataset import collate_fn  # Custom collate function\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        # Define all the necessary options with default values\n",
    "        self.dataroot = \"datasets/human_class\"  # Dataset root directory\n",
    "        self.name = \"human_class\"\n",
    "        self.phase = \"train\"  # Phase (train/test)\n",
    "        self.batch_size = 8  # Batch size\n",
    "        self.ninput_edges = 40000  # Number of input edges for mesh\n",
    "        self.num_threads = 4  # Number of workers for data loading\n",
    "        self.gpu_ids = [0]  # Use GPU (set to [] for CPU)\n",
    "        self.max_dataset_size = float('inf')  # Maximum dataset size\n",
    "        self.serial_batches = False  # Shuffle data (False means shuffle)\n",
    "        self.export_folder = None  # Folder for export results (if any)\n",
    "        self.save_latest_freq = 1000  # Frequency of saving model\n",
    "        self.print_freq = 100  # Frequency of printing log messages\n",
    "        self.epoch_count = 1  # Start counting from which epoch\n",
    "        self.niter = 50  # Number of iterations at base learning rate\n",
    "        self.niter_decay = 50  # Number of iterations with learning rate decay\n",
    "        self.save_epoch_freq = 10  # Frequency of saving model at each epoch\n",
    "        self.verbose_plot = False  # Verbose plotting (e.g. weight plots)\n",
    "        self.run_test_freq = 5  # Frequency to run tests\n",
    "        self.num_aug = 20  # Number of augmentations  \n",
    "        self.checkpoints_dir = \"checkpoints\"\n",
    "\n",
    "# Initialize the Options object\n",
    "opt = Options()\n",
    "\n",
    "# Now we can access the options like opt.gpu_ids, opt.batch_size, etc.\n",
    "print(\"Using GPU IDs:\", opt.gpu_ids)\n",
    "print(\"Batch size:\", opt.batch_size)\n",
    "\n",
    "# Then use `opt` just like you would in the original code\n",
    "\n",
    "# Manually create dataset object using `opt`\n",
    "dataset = ClassificationData(opt)\n",
    "\n",
    "# Create DataLoader with all options specified in `opt`\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=not opt.serial_batches,\n",
    "    num_workers=opt.num_threads,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import networks\n",
    "from os.path import join\n",
    "from util.util import seg_accuracy, print_network\n",
    "\n",
    "class ClassifierModel:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.is_train = opt.is_train\n",
    "        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n",
    "        self.save_dir = join(opt.checkpoints_dir, opt.name)\n",
    "        self.optimizer = None\n",
    "        self.edge_features = None\n",
    "        self.labels = None\n",
    "        self.mesh = None\n",
    "        self.soft_label = None\n",
    "        self.loss = None\n",
    "        self.nclasses = opt.nclasses\n",
    "\n",
    "        norm_layer = networks.get_norm_layer(norm_type=opt.norm, num_groups=opt.num_groups)\n",
    "        net = MeshConvNet(norm_layer, opt.input_nc, opt.ncf, self.nclasses, opt.ninput_edges, opt.pool_res, opt.fc_n, opt.resblocks)\n",
    "        self.net = networks.init_net(net, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "        \n",
    "        self.net.train(self.is_train)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.optimizer = torch.optim.Adam(self.net.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.scheduler = networks.get_scheduler(self.optimizer, opt)\n",
    "            print_network(self.net)\n",
    "\n",
    "        if not self.is_train or opt.continue_train:\n",
    "            self.load_network(opt.which_epoch)\n",
    "\n",
    "    def set_input(self, data):\n",
    "        input_edge_features = torch.from_numpy(data['edge_features']).float()\n",
    "        labels = torch.from_numpy(data['label']).long()\n",
    "        # set inputs\n",
    "        self.edge_features = input_edge_features.to(self.device).requires_grad_(self.is_train)\n",
    "        self.labels = labels.to(self.device)\n",
    "        self.mesh = data['mesh']\n",
    "        if self.opt.dataset_mode == 'segmentation' and not self.is_train:\n",
    "            self.soft_label = torch.from_numpy(data['soft_label'])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        out = self.net(self.edge_features, self.mesh)\n",
    "        return out\n",
    "\n",
    "    def backward(self, out):\n",
    "        self.loss = self.criterion(out, self.labels)\n",
    "        self.loss.backward()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.forward()\n",
    "        self.backward(out)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def load_network(self, which_epoch):\n",
    "        \"\"\"load model from disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % which_epoch\n",
    "        load_path = join(self.save_dir, save_filename)\n",
    "        net = self.net\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net = net.module\n",
    "        print('loading the model from %s' % load_path)\n",
    "        state_dict = torch.load(load_path, map_location=str(self.device))\n",
    "        if hasattr(state_dict, '_metadata'):\n",
    "            del state_dict._metadata\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    def save_network(self, which_epoch):\n",
    "        \"\"\"save model to disk\"\"\"\n",
    "        save_filename = '%s_net.pth' % (which_epoch)\n",
    "        save_path = join(self.save_dir, save_filename)\n",
    "        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "            torch.save(self.net.module.cpu().state_dict(), save_path)\n",
    "            self.net.cuda(self.gpu_ids[0])\n",
    "        else:\n",
    "            torch.save(self.net.cpu().state_dict(), save_path)\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"update learning rate (called once every epoch)\"\"\"\n",
    "        self.scheduler.step()\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"tests model\n",
    "        returns: number correct and total number\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.forward()\n",
    "            # compute number of correct\n",
    "            pred_class = out.data.max(1)[1]\n",
    "            label_class = self.labels\n",
    "            self.export_segmentation(pred_class.cpu())\n",
    "            correct = self.get_accuracy(pred_class, label_class)\n",
    "        return correct, len(label_class)\n",
    "\n",
    "    def get_accuracy(self, pred, labels):\n",
    "        \"\"\"computes accuracy for classification / segmentation \"\"\"\n",
    "        if self.opt.dataset_mode == 'classification':\n",
    "            correct = pred.eq(labels).sum()\n",
    "        elif self.opt.dataset_mode == 'segmentation':\n",
    "            correct = seg_accuracy(pred, self.soft_label, self.mesh)\n",
    "        return correct\n",
    "\n",
    "    def export_segmentation(self, pred_seg):\n",
    "        if self.opt.dataset_mode == 'segmentation':\n",
    "            for meshi, mesh in enumerate(self.mesh):\n",
    "                mesh.export_segments(pred_seg[meshi, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
